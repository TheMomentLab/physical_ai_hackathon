# HIL-SERL ì‹¤ì œ ë¡œë´‡ í•™ìŠµ ì›Œí¬í”Œë¡œ ê°€ì´ë“œ

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” LeRobotì„ ì‚¬ìš©í•´ Human-in-the-Loop Sample-Efficient Reinforcement Learning(HIL-SERL)ì˜ ì „ì²´ ì›Œí¬í”Œë¡œë¥¼ ë”°ë¼ê°€ë©°, ì‹¤ì œ ë¡œë´‡ì—ì„œ RL ì •ì±…ì„ ëª‡ ì‹œê°„ ë§Œì— í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ìµí™ë‹ˆë‹¤.

HIL-SERLì€ ì‚¬ëŒì˜ ë°ëª¨ì™€ ì˜¨ë¼ì¸ í•™ìŠµ, ê·¸ë¦¬ê³  ì¸ê°„ ê°œì…ì„ ê²°í•©í•œ ìƒ˜í”Œ íš¨ìœ¨ì  ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì†ŒëŸ‰ì˜ ì¸ê°„ ë°ëª¨ë¡œ ì‹œì‘í•´ ë³´ìƒ ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•˜ê³ , ì´í›„ actor-learner êµ¬ì¡°ì—ì„œ ì •ì±… ì‹¤í–‰ ì¤‘ ì‚¬ëŒì´ ê°œì…í•´ íƒìƒ‰ì„ ìœ ë„í•˜ê³  ìœ„í—˜í•œ í–‰ë™ì„ ë°”ë¡œì¡ìŠµë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ê²Œì„íŒ¨ë“œë¡œ ê°œì…ì„ ì œê³µí•˜ê³  í•™ìŠµ ê³¼ì • ë™ì•ˆ ë¡œë´‡ì„ ì œì–´í•©ë‹ˆë‹¤.

HIL-SERLì€ ë‹¤ìŒ ì„¸ ê°€ì§€ í•µì‹¬ ìš”ì†Œë¥¼ ê²°í•©í•©ë‹ˆë‹¤:

1. **ì˜¤í”„ë¼ì¸ ë°ëª¨ & ë³´ìƒ ë¶„ë¥˜ê¸°:** ì†Œìˆ˜ì˜ ì¸ê°„ í…”ë ˆì˜¤í¼ë ˆì´ì…˜ ì—í”¼ì†Œë“œì™€ ì‹œê° ê¸°ë°˜ ì„±ê³µ ê°ì§€ê¸°ë¥¼ ì‚¬ìš©í•´ ì •ì±…ì˜ ì¶œë°œì ì„ ì œê³µ
2. **ë¡œë´‡ ìƒì˜ actor / learner ë£¨í”„ + ì¸ê°„ ê°œì…:** ë¶„ì‚° SAC í•™ìŠµê¸°ê°€ ì •ì±…ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë™ì•ˆ, actorê°€ ì‹¤ì œ ë¡œë´‡ì—ì„œ íƒìƒ‰ì„ ìˆ˜í–‰í•˜ë©° ì‚¬ëŒì€ ì–¸ì œë“  ê°œì… ê°€ëŠ¥
3. **ì•ˆì „ & íš¨ìœ¨ ë„êµ¬:** ê´€ì ˆ/ì—”ë“œì´í™í„°(EE) ê²½ê³„, ROI(ê´€ì‹¬ì˜ì—­) ì „ì²˜ë¦¬, WandB ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë°ì´í„° í’ˆì§ˆê³¼ í•˜ë“œì›¨ì–´ ì•ˆì „ í™•ë³´

ì´ ìš”ì†Œë“¤ì„ ê²°í•©í•˜ë©´, HIL-SERLì€ ëª¨ë°© í•™ìŠµë§Œ ì‚¬ìš©í•œ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ë” ë¹ ë¥¸ ì‚¬ì´í´ê³¼ ë†’ì€ ì„±ê³µë¥ ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

HIL-SERL ì›Œí¬í”Œë¡œ, Luo et al. 2024

ì´ ê°€ì´ë“œëŠ” LeRobotì˜ HilSerl êµ¬í˜„ì„ ì´ìš©í•´ ì‹¤ì œ ë¡œë´‡ì—ì„œ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

## ì¤€ë¹„ë¬¼

- ê²Œì„íŒ¨ë“œ(ê¶Œì¥) ë˜ëŠ” í‚¤ë³´ë“œ
- Nvidia GPU
- ì‹¤ì œ ë¡œë´‡(íŒ”ë¡œì›Œ/ë¦¬ë” ì•”ì´ ìˆìœ¼ë©´ ì¢‹ìŒ. í‚¤ë³´ë“œë‚˜ ê²Œì„íŒ¨ë“œë¥¼ ì“°ë©´ í•„ìˆ˜ëŠ” ì•„ë‹˜)
- ë¡œë´‡ìš© URDF íŒŒì¼(kinematics íŒ¨í‚¤ì§€ì—ì„œ ì‚¬ìš©. `lerobot/model/kinematics.py` ì°¸ê³ )

## ì–´ë–¤ ì‘ì—…ì„ í•™ìŠµí•  ìˆ˜ ìˆë‚˜ìš”?

HIL-SERLì€ ë‹¤ì–‘í•œ ì¡°ì‘ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ì²œ ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- ì‹œìŠ¤í…œ ë™ì‘ì„ ì´í•´í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•œ ì‘ì—…ë¶€í„° ì‹œì‘í•˜ì„¸ìš”.
  - íë¸Œë¥¼ ëª©í‘œ ì˜ì—­ìœ¼ë¡œ ë°€ê¸°
  - ê·¸ë¦¬í¼ë¡œ íë¸Œë¥¼ ì§‘ì–´ì„œ ë“¤ì–´ ì˜¬ë¦¬ê¸°
- ë„ˆë¬´ ê¸´ horizon ì‘ì—…ì€ í”¼í•˜ì„¸ìš”. 5~10ì´ˆ ë‚´ì— ì™„ë£Œ ê°€ëŠ¥í•œ ì‘ì—…ì— ì§‘ì¤‘í•˜ì„¸ìš”.
- ì‹œìŠ¤í…œ ì´í•´ê°€ ì¶©ë¶„í•´ì§€ë©´ ë” ë³µì¡í•˜ê³  ê¸´ ì‘ì—…ì„ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - íë¸Œ ì§‘ì–´ì„œ ë†“ê¸°
  - ì–‘íŒ”ë¡œ ë¬¼ì²´ ì§‘ê¸° ê°™ì€ ì–‘íŒ” ì‘ì—…
  - í•œ íŒ”ì—ì„œ ë‹¤ë¥¸ íŒ”ë¡œ ë¬¼ì²´ë¥¼ ì „ë‹¬í•˜ëŠ” hand-over ì‘ì—…
  - ë§ˆìŒê» í™•ì¥!

## HIL-SERL í¬í•¨ LeRobot ì„¤ì¹˜

HIL-SERLì„ ì‚¬ìš©í•˜ë ¤ë©´ `hilserl` extraë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
pip install -e ".[hilserl]"
```

## ì‹¤ì œ ë¡œë´‡ í•™ìŠµ ì›Œí¬í”Œë¡œ

### ì„¤ì • ì´í•´í•˜ê¸°

í•™ìŠµ ê³¼ì •ì€ HILSerl í™˜ê²½ì„ ìœ„í•œ ì˜¬ë°”ë¥¸ ì„¤ì •ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤. í•µì‹¬ ì„¤ì • í´ë˜ìŠ¤ëŠ” `lerobot/rl/gym_manipulator.py`ì˜ `GymManipulatorConfig`ì´ë©°, ë‚´ë¶€ì— `HILSerlRobotEnvConfig`ì™€ `DatasetConfig`ê°€ ì¤‘ì²©ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì„¤ì •ì€ ê¸°ëŠ¥ë³„ ì„œë¸Œ êµ¬ì„±ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤:

```python
class GymManipulatorConfig:
    env: HILSerlRobotEnvConfig    # í™˜ê²½ ì„¤ì • (ì¤‘ì²©)
    dataset: DatasetConfig    # ë°ì´í„°ì…‹ ê¸°ë¡/ë¦¬í”Œë ˆì´ ì„¤ì • (ì¤‘ì²©)
    mode: str | None = None    # "record", "replay", ë˜ëŠ” None(í•™ìŠµ)
    device: str = "cpu"    # ì—°ì‚° ì¥ì¹˜

class HILSerlRobotEnvConfig(EnvConfig):
    robot: RobotConfig | None = None    # ë©”ì¸ ë¡œë´‡ ì—ì´ì „íŠ¸ (`lerobot/robots`)
    teleop: TeleoperatorConfig | None = None    # í…”ë ˆì˜¤í¼ë ˆì´í„°(ì˜ˆ: ê²Œì„íŒ¨ë“œ, ë¦¬ë” ì•”)
    processor: HILSerlProcessorConfig    # ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì • (ì¤‘ì²©)
    name: str = "real_robot"    # í™˜ê²½ ì´ë¦„
    task: str | None = None    # íƒœìŠ¤í¬ ì‹ë³„ì
    fps: int = 10    # ì œì–´ ì£¼íŒŒìˆ˜

# ì¤‘ì²© í”„ë¡œì„¸ì„œ êµ¬ì„±
class HILSerlProcessorConfig:
    control_mode: str = "gamepad"    # ì œì–´ ëª¨ë“œ
    observation: ObservationConfig | None = None    # ê´€ì¸¡ ì²˜ë¦¬ ì„¤ì •
    image_preprocessing: ImagePreprocessingConfig | None = None    # ì´ë¯¸ì§€ í¬ë¡­/ë¦¬ì‚¬ì´ì¦ˆ ì„¤ì •
    gripper: GripperConfig | None = None    # ê·¸ë¦¬í¼ ì œì–´/íŒ¨ë„í‹° ì„¤ì •
    reset: ResetConfig | None = None    # ë¦¬ì…‹ ë° íƒ€ì´ë° ì„¤ì •
    inverse_kinematics: InverseKinematicsConfig | None = None    # IK ì„¤ì •
    reward_classifier: RewardClassifierConfig | None = None    # ë³´ìƒ ë¶„ë¥˜ê¸° ì„¤ì •
    max_gripper_pos: float | None = 100.0    # ê·¸ë¦¬í¼ ìµœëŒ€ ìœ„ì¹˜

# í•˜ìœ„ êµ¬ì„± í´ë˜ìŠ¤
class ObservationConfig:
    add_joint_velocity_to_observation: bool = False    # ê´€ì¸¡ì— ê´€ì ˆ ì†ë„ ì¶”ê°€
    add_current_to_observation: bool = False    # ê´€ì¸¡ì— ëª¨í„° ì „ë¥˜ ì¶”ê°€
    display_cameras: bool = False    # ì‹¤í–‰ ì¤‘ ì¹´ë©”ë¼ í”¼ë“œ í‘œì‹œ

class ImagePreprocessingConfig:
    crop_params_dict: dict[str, tuple[int, int, int, int]] | None = None    # ì´ë¯¸ì§€ í¬ë¡­ íŒŒë¼ë¯¸í„°
    resize_size: tuple[int, int] | None = None    # ëª©í‘œ ì´ë¯¸ì§€ í¬ê¸°

class GripperConfig:
    use_gripper: bool = True    # ê·¸ë¦¬í¼ ì‚¬ìš©
    gripper_penalty: float = 0.0    # ë¶€ì ì ˆí•œ ê·¸ë¦¬í¼ ì‚¬ìš© íŒ¨ë„í‹°

class ResetConfig:
    fixed_reset_joint_positions: Any | None = None    # ë¦¬ì…‹ ê´€ì ˆ ìœ„ì¹˜
    reset_time_s: float = 5.0    # ë¦¬ì…‹ ëŒ€ê¸° ì‹œê°„
    control_time_s: float = 20.0    # ìµœëŒ€ ì—í”¼ì†Œë“œ ê¸¸ì´
    terminate_on_success: bool = True    # ì„±ê³µ ì‹œ ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€

class InverseKinematicsConfig:
    urdf_path: str | None = None    # URDF ê²½ë¡œ
    target_frame_name: str | None = None    # EE í”„ë ˆì„ ì´ë¦„
    end_effector_bounds: dict[str, list[float]] | None = None    # EE ì‘ì—…ê³µê°„ ê²½ê³„
    end_effector_step_sizes: dict[str, float] | None = None    # EE ì¶•ë³„ ìŠ¤í… í¬ê¸°

class RewardClassifierConfig:
    pretrained_path: str | None = None    # ì‚¬ì „í•™ìŠµ ë³´ìƒ ë¶„ë¥˜ê¸° ê²½ë¡œ
    success_threshold: float = 0.5    # ì„±ê³µ íŒì • ì„ê³„ê°’
    success_reward: float = 1.0    # ì„±ê³µ ë³´ìƒ ê°’

# ë°ì´í„°ì…‹ ì„¤ì •
class DatasetConfig:
    repo_id: str    # LeRobot ë°ì´í„°ì…‹ ì €ì¥ì†Œ ID
    task: str    # íƒœìŠ¤í¬ ì‹ë³„ì
    root: str | None = None    # ë¡œì»¬ ë°ì´í„°ì…‹ ë£¨íŠ¸
    num_episodes_to_record: int = 5    # ê¸°ë¡í•  ì—í”¼ì†Œë“œ ìˆ˜
    replay_episode: int | None = None    # ë¦¬í”Œë ˆì´ ì—í”¼ì†Œë“œ ì¸ë±ìŠ¤
    push_to_hub: bool = False    # Hub ì—…ë¡œë“œ ì—¬ë¶€
```

### í”„ë¡œì„¸ì„œ íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜

HIL-SERLì€ ëª¨ë“ˆí˜• í”„ë¡œì„¸ì„œ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•´ ë¡œë´‡ ê´€ì¸¡ê³¼ ì•¡ì…˜ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì€ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

#### í™˜ê²½ í”„ë¡œì„¸ì„œ íŒŒì´í”„ë¼ì¸

í™˜ê²½ í”„ë¡œì„¸ì„œ(`env_processor`)ëŠ” ë“¤ì–´ì˜¤ëŠ” ê´€ì¸¡ê³¼ ìƒíƒœë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:

1. **VanillaObservationProcessorStep**: ì›ì‹œ ê´€ì¸¡ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
2. **JointVelocityProcessorStep**(ì˜µì…˜): ê´€ì¸¡ì— ê´€ì ˆ ì†ë„ ì¶”ê°€
3. **MotorCurrentProcessorStep**(ì˜µì…˜): ê´€ì¸¡ì— ëª¨í„° ì „ë¥˜ ì¶”ê°€
4. **ForwardKinematicsJointsToEE**(ì˜µì…˜): ê´€ì ˆì—ì„œ EE í¬ì¦ˆ ê³„ì‚°
5. **ImageCropResizeProcessorStep**(ì˜µì…˜): ì´ë¯¸ì§€ í¬ë¡­/ë¦¬ì‚¬ì´ì¦ˆ
6. **TimeLimitProcessorStep**(ì˜µì…˜): ì—í”¼ì†Œë“œ ì‹œê°„ ì œí•œ
7. **GripperPenaltyProcessorStep**(ì˜µì…˜): ê·¸ë¦¬í¼ ì‚¬ìš© íŒ¨ë„í‹°
8. **RewardClassifierProcessorStep**(ì˜µì…˜): ë¹„ì „ ê¸°ë°˜ ë³´ìƒ ê°ì§€
9. **AddBatchDimensionProcessorStep**: ë°°ì¹˜ ì°¨ì› ì¶”ê°€
10. **DeviceProcessorStep**: ë°ì´í„°ë¥¼ ì§€ì •í•œ ì¥ì¹˜(CPU/GPU)ë¡œ ì´ë™

#### ì•¡ì…˜ í”„ë¡œì„¸ì„œ íŒŒì´í”„ë¼ì¸

ì•¡ì…˜ í”„ë¡œì„¸ì„œ(`action_processor`)ëŠ” ë‚˜ê°€ëŠ” ì•¡ì…˜ê³¼ ì¸ê°„ ê°œì…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤:

1. **AddTeleopActionAsComplimentaryDataStep**: í…”ë ˆì˜¤í¼ë ˆì´í„° ì•¡ì…˜ ê¸°ë¡
2. **AddTeleopEventsAsInfoStep**: ê°œì… ì´ë²¤íŠ¸ ë° ì—í”¼ì†Œë“œ ì œì–´ ê¸°ë¡
3. **InterventionActionProcessorStep**: ì¸ê°„ ê°œì… ë° ì¢…ë£Œ ì²˜ë¦¬
4. **ì—­ê¸°êµ¬í•™ íŒŒì´í”„ë¼ì¸**(í™œì„± ì‹œ):
   - **MapDeltaActionToRobotActionStep**: ë¸íƒ€ ì•¡ì…˜ì„ ë¡œë´‡ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
   - **EEReferenceAndDelta**: EE ê¸°ì¤€ ë° ë¸íƒ€ ê³„ì‚°
   - **EEBoundsAndSafety**: ì‘ì—…ê³µê°„ ì•ˆì „ ê²½ê³„ ì ìš©
   - **InverseKinematicsEEToJoints**: EE ì•¡ì…˜ì„ ê´€ì ˆ íƒ€ê²Ÿìœ¼ë¡œ ë³€í™˜
   - **GripperVelocityToJoint**: ê·¸ë¦¬í¼ ì œì–´ ì²˜ë¦¬

#### ì„¤ì • ì˜ˆì‹œ

**ê¸°ë³¸ ê´€ì¸¡ ì²˜ë¦¬**:

```json
{
  "env": {
    "processor": {
      "observation": {
        "add_joint_velocity_to_observation": true,
        "add_current_to_observation": false,
        "display_cameras": false
      }
    }
  }
}
```

**ì´ë¯¸ì§€ ì²˜ë¦¬**:

```json
{
  "env": {
    "processor": {
      "image_preprocessing": {
        "crop_params_dict": {
          "observation.images.front": [180, 250, 120, 150],
          "observation.images.side": [180, 207, 180, 200]
        },
        "resize_size": [128, 128]
      }
    }
  }
}
```

**ì—­ê¸°êµ¬í•™ ì„¤ì •**:

```json
{
  "env": {
    "processor": {
      "inverse_kinematics": {
        "urdf_path": "path/to/robot.urdf",
        "target_frame_name": "end_effector",
        "end_effector_bounds": {
          "min": [0.16, -0.08, 0.03],
          "max": [0.24, 0.2, 0.1]
        },
        "end_effector_step_sizes": {
          "x": 0.02,
          "y": 0.02,
          "z": 0.02
        }
      }
    }
  }
}
```

### ê³ ê¸‰ ê´€ì¸¡ ì²˜ë¦¬

HIL-SERLì€ ê´€ì¸¡ ì²˜ë¦¬ì—ì„œ ì¶”ê°€ ê¸°ëŠ¥ì„ ì§€ì›í•˜ë©°, ì •ì±… í•™ìŠµì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ê´€ì ˆ ì†ë„ ì²˜ë¦¬

ê´€ì ˆ ì†ë„ ì¶”ì •ì„ í™œì„±í™”í•˜ì—¬ ì •ì±…ì— ë™ì‘ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```json
{
  "env": {
    "processor": {
      "observation": {
        "add_joint_velocity_to_observation": true
      }
    }
  }
}
```

ì´ í”„ë¡œì„¸ì„œëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- ì—°ì†ëœ ê´€ì ˆ ìœ„ì¹˜ ì°¨ë¶„ìœ¼ë¡œ ì†ë„ ì¶”ì •
- ê´€ì¸¡ ìƒíƒœ ë²¡í„°ì— ì†ë„ ì¶”ê°€
- ë™ì  ì‘ì—…ì—ì„œ ìœ ìš©

#### ëª¨í„° ì „ë¥˜ ì²˜ë¦¬

ëª¨í„° ì „ë¥˜ë¥¼ ëª¨ë‹ˆí„°ë§í•´ ì ‘ì´‰ í˜ì´ë‚˜ ë¶€í•˜ë¥¼ ê°ì§€í•©ë‹ˆë‹¤:

```json
{
  "env": {
    "processor": {
      "observation": {
        "add_current_to_observation": true
      }
    }
  }
}
```

ì´ í”„ë¡œì„¸ì„œëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œì—ì„œ ì „ë¥˜ ê°’ ì½ê¸°
- ê´€ì¸¡ ìƒíƒœ ë²¡í„°ì— ì „ë¥˜ ì¶”ê°€
- ì ‘ì´‰ ì´ë²¤íŠ¸/ë¬´ê²Œ/ì €í•­ ê°ì§€ì— ë„ì›€
- ì ‘ì´‰ì´ ë§ì€ ì‘ì—…ì— ìœ ìš©

#### ê´€ì¸¡ ì²˜ë¦¬ ì¡°í•©

ì—¬ëŸ¬ ê´€ì¸¡ ì²˜ë¦¬ë¥¼ ë™ì‹œì— í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```json
{
  "env": {
    "processor": {
      "observation": {
        "add_joint_velocity_to_observation": true,
        "add_current_to_observation": true,
        "display_cameras": false
      }
    }
  }
}
```

**Note**: ê´€ì¸¡ì„ ì¶”ê°€í•˜ë©´ ìƒíƒœ ê³µê°„ ì°¨ì›ì´ ì¦ê°€í•˜ë¯€ë¡œ, ì •ì±… ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì¡°ì •ì´ë‚˜ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë¡œë´‡ ì‘ì—…ê³µê°„ ê²½ê³„ ì°¾ê¸°

ë°ëª¨ ìˆ˜ì§‘ ì „ ë¡œë´‡ì˜ ì‘ì—…ê³µê°„ ê²½ê³„ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ì´ëŠ” ì‹¤ì œ ë¡œë´‡ í•™ìŠµì„ ë‹¨ìˆœí™”í•˜ëŠ” ë° ë‘ ê°€ì§€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤: 1) ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” íŠ¹ì • ì˜ì—­ìœ¼ë¡œ ì‘ì—… ê³µê°„ì„ ì œí•œí•´ ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ìœ„í—˜í•œ íƒìƒ‰ì„ ì¤„ì´ê³ , 2) ê´€ì ˆ ê³µê°„ì´ ì•„ë‹Œ ì—”ë“œì´í™í„°(EE) ê³µê°„ì—ì„œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ê²½í—˜ì ìœ¼ë¡œ ì¡°ì‘ ì‘ì—…ì—ì„œ RLì„ ê´€ì ˆ ê³µê°„ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë” ì–´ë µê³ , ì–´ë–¤ ì‘ì—…ì€ EE ê³µê°„ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

**`lerobot-find-joint-limits` ì‚¬ìš©**

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¡œë´‡ EEì˜ ì•ˆì „í•œ ì‘ì—… ê²½ê³„ë¥¼ ì°¾ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. íŒ”ë¡œì›Œ/ë¦¬ë” ì•”ì´ ìˆë‹¤ë©´ íŒ”ë¡œì›Œ ì•”ì˜ ê²½ê³„ë¥¼ ì°¾ì•„ í•™ìŠµì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•¡ì…˜ ê³µê°„ì„ ì œí•œí•˜ë©´ ë¶ˆí•„ìš”í•œ íƒìƒ‰ì„ ì¤„ì´ê³  ì•ˆì „ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

```bash
lerobot-find-joint-limits \
  --robot.type=so100_follower \
  --robot.port=/dev/follower_arm_1 \
  --robot.id=black \
  --teleop.type=so100_leader \
  --teleop.port=/dev/leader_arm_1 \
  --teleop.id=blue
```

**ì›Œí¬í”Œë¡œ**

1. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ì‘ì—…ì— í•„ìš”í•œ ê³µê°„ìœ¼ë¡œ ë¡œë´‡ì„ ì›€ì§ì…ë‹ˆë‹¤.
2. ìŠ¤í¬ë¦½íŠ¸ê°€ EE ìœ„ì¹˜ì™€ ê´€ì ˆ ê°ë„ì˜ ìµœì†Œ/ìµœëŒ€ ê°’ì„ ê¸°ë¡í•´ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤. ì˜ˆ:
   ```
   Max ee position [0.2417 0.2012 0.1027]
   Min ee position [0.1663 -0.0823 0.0336]
   Max joint positions [-20.0, -20.0, -20.0, -20.0, -20.0, -20.0]
   Min joint positions [50.0, 50.0, 50.0, 50.0, 50.0, 50.0]
   ```
3. ì´ ê°’ì„ í…”ë ˆì˜¤í¼ë ˆì´í„° ì„¤ì •(`TeleoperatorConfig`)ì˜ `end_effector_bounds`ì— ì…ë ¥í•©ë‹ˆë‹¤.

**ì˜ˆì‹œ ì„¤ì •**

```json
"end_effector_bounds": {
    "max": [0.24, 0.20, 0.10],
    "min": [0.16, -0.08, 0.03]
}
```

### ë°ëª¨ ìˆ˜ì§‘

ê²½ê³„ê°€ ì„¤ì •ë˜ë©´ ì•ˆì „í•˜ê²Œ ë°ëª¨ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤í”„í´ë¦¬ì‹œ RLì€ ì˜¤í”„ë¼ì¸ ë°ì´í„°ì…‹ì„ í™œìš©í•´ í•™ìŠµ íš¨ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Record ëª¨ë“œ ì„¤ì •**

ë°ëª¨ ê¸°ë¡ì„ ìœ„í•œ ì„¤ì • íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤(ì˜ˆ: [env_config.json](https://huggingface.co/datasets/lerobot/config_examples/resolve/main/rl/env_config.json)).

1. ë£¨íŠ¸ì—ì„œ `mode`ë¥¼ `"record"`ë¡œ ì„¤ì •
2. `dataset.repo_id`ì— ê³ ìœ í•œ ì €ì¥ì†Œ ì´ë¦„ ì§€ì •(ì˜ˆ: `username/task_name`)
3. `dataset.num_episodes_to_record`ì— ìˆ˜ì§‘í•  ì—í”¼ì†Œë“œ ìˆ˜ ì§€ì •
4. `env.processor.image_preprocessing.crop_params_dict`ë¥¼ `{}`ë¡œ ì´ˆê¸° ì„¤ì •(ë‚˜ì¤‘ì— í¬ë¡­ ê²°ì •)
5. `env` ì„¹ì…˜ì—ì„œ `robot`, `teleop` ë“± í•˜ë“œì›¨ì–´ ì„¤ì • êµ¬ì„±

ì˜ˆì‹œ ì„¤ì • ì„¹ì…˜:

```json
{
  "env": {
    "type": "gym_manipulator",
    "name": "real_robot",
    "fps": 10,
    "processor": {
      "control_mode": "gamepad",
      "observation": {
        "display_cameras": false
      },
      "image_preprocessing": {
        "crop_params_dict": {},
        "resize_size": [128, 128]
      },
      "gripper": {
        "use_gripper": true,
        "gripper_penalty": 0.0
      },
      "reset": {
        "reset_time_s": 5.0,
        "control_time_s": 20.0
      }
    },
    "robot": {
      // ... robot configuration ...
    },
    "teleop": {
      // ... teleoperator configuration ...
    }
  },
  "dataset": {
    "repo_id": "username/pick_lift_cube",
    "root": null,
    "task": "pick_and_lift",
    "num_episodes_to_record": 15,
    "replay_episode": 0,
    "push_to_hub": true
  },
  "mode": "record",
  "device": "cpu"
}
```

### í…”ë ˆì˜¤í¼ë ˆì´ì…˜ ì¥ì¹˜ ì‚¬ìš©

ë°ì´í„° ìˆ˜ì§‘ê³¼ ì˜¨ë¼ì¸ í•™ìŠµ ì¤‘ ê°œì…ì„ ìœ„í•´ í…”ë ˆì˜¤í¼ë ˆì´ì…˜ ì¥ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ê²Œì„íŒ¨ë“œ, í‚¤ë³´ë“œ, ë˜ëŠ” ë¦¬ë” ì•”ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

HIL-Serlì€ ë¡œë´‡ì˜ EE ê³µê°„ì—ì„œ ì•¡ì…˜ì„ í•™ìŠµí•©ë‹ˆë‹¤. ë”°ë¼ì„œ í…”ë ˆì˜¤í¼ë ˆì´ì…˜ì€ EEì˜ x, y, z ë³€ìœ„ë¥¼ ì œì–´í•©ë‹ˆë‹¤.

ì´ë¥¼ ìœ„í•´ EE ê³µê°„ì—ì„œ ì•¡ì…˜ì„ ë°›ëŠ” ë¡œë´‡ ë²„ì „ì´ í•„ìš”í•©ë‹ˆë‹¤. ê¸°ë³¸ íŒŒë¼ë¯¸í„°ëŠ” `SO100FollowerEndEffector`ì™€ `SO100FollowerEndEffectorConfig`ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

```python
class SO100FollowerEndEffectorConfig(SO100FollowerConfig):
    """SO100FollowerEndEffector ë¡œë´‡ ì„¤ì •."""

    # EE ìœ„ì¹˜ ê¸°ë³¸ ê²½ê³„(ë¯¸í„°)
    end_effector_bounds: dict[str, list[float]] = field( # x,y,z ê²½ê³„
        default_factory=lambda: {
            "min": [-1.0, -1.0, -1.0],  # min x, y, z
            "max": [1.0, 1.0, 1.0],  # max x, y, z
        }
    )

    max_gripper_pos: float = 50 # ê·¸ë¦¬í¼ ìµœëŒ€ ì˜¤í”ˆ ìœ„ì¹˜

    end_effector_step_sizes: dict[str, float] = field( # EE x,y,z ìŠ¤í… í¬ê¸°
        default_factory=lambda: {
            "x": 0.02,
            "y": 0.02,
            "z": 0.02,
        }
    )
```

`Teleoperator`ëŠ” í…”ë ˆì˜¤í¼ë ˆì´ì…˜ ì¥ì¹˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ í…”ë ˆì˜¤í¼ë ˆì´í„° ëª©ë¡ì€ `lerobot/teleoperators`ì—ì„œ í™•ì¸í•˜ì„¸ìš”.

**ê²Œì„íŒ¨ë“œ ì„¤ì •**

ê²Œì„íŒ¨ë“œëŠ” ë¡œë´‡ê³¼ ì—í”¼ì†Œë“œ ìƒíƒœë¥¼ ì œì–´í•˜ê¸°ì— ë§¤ìš° í¸ë¦¬í•©ë‹ˆë‹¤.

ê²Œì„íŒ¨ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `control_mode`ë¥¼ `"gamepad"`ë¡œ ì„¤ì •í•˜ê³  `teleop` ì„¹ì…˜ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

```json
{
  "env": {
    "teleop": {
      "type": "gamepad",
      "use_gripper": true
    },
    "processor": {
      "control_mode": "gamepad",
      "gripper": {
        "use_gripper": true
      }
    }
  }
}
```

Gamepad button mapping for robot control and episode management

**SO101 ë¦¬ë” ì„¤ì •**

SO101 ë¦¬ë” ì•”ì€ ê°ì† ê¸°ì–´ê°€ ìˆì–´ íŒ”ë¡œì›Œ ì•”ì„ ë” ë¶€ë“œëŸ½ê²Œ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê¸°ì–´ê°€ ì—†ëŠ” SO100ë³´ë‹¤ ê°œì…ì´ ë§¤ë„ëŸ½ìŠµë‹ˆë‹¤.

SO101 ë¦¬ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `control_mode`ë¥¼ `"leader"`ë¡œ ì„¤ì •í•˜ê³  `teleop` ì„¹ì…˜ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

```json
{
  "env": {
    "teleop": {
      "type": "so101_leader",
      "port": "/dev/follower_arm_1",
      "use_degrees": true
    },
    "processor": {
      "control_mode": "leader",
      "gripper": {
        "use_gripper": true
      }
    }
  }
}
```

ì„±ê³µ/ì‹¤íŒ¨ ë¼ë²¨ë§ì„ ìœ„í•´ **í‚¤ë³´ë“œ**ë¡œ `s`(ì„±ê³µ), `esc`(ì‹¤íŒ¨)ë¥¼ ëˆŒëŸ¬ì•¼ í•©ë‹ˆë‹¤.
ì˜¨ë¼ì¸ í•™ìŠµ ì¤‘ì—ëŠ” `space`ë¡œ ì •ì±…ì„ ì¼ì‹œ ì¤‘ì§€í•˜ê³  ê°œì…í•˜ë©°, ë‹¤ì‹œ `space`ë¡œ ì •ì±…ì— ì œì–´ë¥¼ ëŒë ¤ì¤ë‹ˆë‹¤.

Video: SO101 leader teleoperation

SO101 leader teleoperation example, the leader tracks the follower, press `space` to intervene

**ë°ëª¨ ê¸°ë¡**

ê¸°ë¡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì˜ˆì‹œ ì„¤ì • íŒŒì¼ì€ [ì—¬ê¸°](https://huggingface.co/datasets/aractingi/lerobot-example-config-files/blob/main/env_config_so100.json)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
python -m lerobot.rl.gym_manipulator --config_path src/lerobot/configs/env_config_so100.json
```

ê¸°ë¡ ì¤‘:

1. ë¡œë´‡ì€ ì„¤ì • íŒŒì¼ì˜ `env.processor.reset.fixed_reset_joint_positions`ë¡œ ë¦¬ì…‹í•©ë‹ˆë‹¤.
2. ì‘ì—…ì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí•©ë‹ˆë‹¤.
3. "ì„±ê³µ" ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ë³´ìƒ 1ë¡œ ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë©ë‹ˆë‹¤.
4. ì‹œê°„ ì œí•œì— ë„ë‹¬í•˜ê±°ë‚˜ "ì‹¤íŒ¨" ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ë³´ìƒ 0ìœ¼ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤.
5. "rerecord" ë²„íŠ¼ìœ¼ë¡œ ì—í”¼ì†Œë“œë¥¼ ë‹¤ì‹œ ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
6. ìë™ìœ¼ë¡œ ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
7. ëª¨ë“  ì—í”¼ì†Œë“œ ê¸°ë¡ í›„, ë°ì´í„°ì…‹ì€ Hubì— ì—…ë¡œë“œ(ì˜µì…˜)ë˜ê³  ë¡œì»¬ì— ì €ì¥ë©ë‹ˆë‹¤.

### ë°ì´í„°ì…‹ ì²˜ë¦¬

ë°ëª¨ ìˆ˜ì§‘ í›„ ì¹´ë©”ë¼ í¬ë¡­ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. RLì€ ë°°ê²½ ë…¸ì´ì¦ˆì— ë¯¼ê°í•˜ë¯€ë¡œ ì‘ì—… ì˜ì—­ì— ì§‘ì¤‘í•˜ë„ë¡ ì´ë¯¸ì§€ë¥¼ í¬ë¡­í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

ë¹„ì „ ê¸°ë°˜ RLì€ í”½ì…€ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ í•™ìŠµí•˜ë¯€ë¡œ ê´€ë ¨ ì—†ëŠ” ì‹œê° ì •ë³´ì— ì·¨ì•½í•©ë‹ˆë‹¤. ì¡°ëª… ë³€í™”, ê·¸ë¦¼ì, ì›€ì§ì´ëŠ” ì‚¬ëŒ, ì‘ì—… ì˜ì—­ ë°– ë¬¼ì²´ ë“±ì€ í•™ìŠµì„ í˜¼ë€ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¢‹ì€ ROIëŠ” ë‹¤ìŒì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:

- ì‘ì—…ì´ ì´ë£¨ì–´ì§€ëŠ” í•„ìˆ˜ ì‘ì—… ê³µê°„ë§Œ í¬í•¨
- ë¡œë´‡ EEì™€ ëª¨ë“  ëŒ€ìƒ ë¬¼ì²´ í¬í•¨
- ë¶ˆí•„ìš”í•œ ë°°ê²½ ìš”ì†Œì™€ ë°©í•´ ìš”ì†Œ ì œì™¸

ì°¸ê³ : ì´ë¯¸ í¬ë¡­ íŒŒë¼ë¯¸í„°ë¥¼ ì•Œê³  ìˆë‹¤ë©´ ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³ , ê¸°ë¡ ë‹¨ê³„ì—ì„œ `crop_params_dict`ì— ë°”ë¡œ ì…ë ¥í•´ë„ ë©ë‹ˆë‹¤.

**í¬ë¡­ íŒŒë¼ë¯¸í„° ê²°ì •**

`crop_dataset_roi.py` ìŠ¤í¬ë¦½íŠ¸ë¡œ ì¹´ë©”ë¼ ì´ë¯¸ì§€ì˜ ê´€ì‹¬ ì˜ì—­ì„ ì„ íƒí•©ë‹ˆë‹¤:

```bash
python -m lerobot.rl.crop_dataset_roi --repo-id username/pick_lift_cube
```

1. ê° ì¹´ë©”ë¼ ë·°ì— ëŒ€í•´ ì²« í”„ë ˆì„ì´ í‘œì‹œë©ë‹ˆë‹¤.
2. ì‘ì—… ì˜ì—­ì„ í¬í•¨í•˜ëŠ” ì‚¬ê°í˜•ì„ ê·¸ë¦½ë‹ˆë‹¤.
3. `c`ë¥¼ ëˆŒëŸ¬ ì„ íƒì„ í™•ì •í•©ë‹ˆë‹¤.
4. ëª¨ë“  ì¹´ë©”ë¼ ë·°ì— ëŒ€í•´ ë°˜ë³µí•©ë‹ˆë‹¤.
5. ìŠ¤í¬ë¦½íŠ¸ê°€ í¬ë¡­ íŒŒë¼ë¯¸í„°ë¥¼ ì¶œë ¥í•˜ê³ , í¬ë¡­ëœ ìƒˆ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì˜ˆì‹œ ì¶œë ¥:

```
Selected Rectangular Regions of Interest (top, left, height, width):
observation.images.side: [180, 207, 180, 200]
observation.images.front: [180, 250, 120, 150]
```

Interactive cropping tool for selecting regions of interest

**ì„¤ì • ì—…ë°ì´íŠ¸**

í•™ìŠµ ì„¤ì •ì— í¬ë¡­ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤:

```json
{
  "env": {
    "processor": {
      "image_preprocessing": {
        "crop_params_dict": {
          "observation.images.side": [180, 207, 180, 200],
          "observation.images.front": [180, 250, 120, 150]
        },
        "resize_size": [128, 128]
      }
    }
  }
}
```

**ê¶Œì¥ ì´ë¯¸ì§€ í•´ìƒë„**

ëŒ€ë¶€ë¶„ì˜ ë¹„ì „ ì •ì±…ì€ **128Ã—128**(ê¸°ë³¸) ë˜ëŠ” **64Ã—64** ì •ì‚¬ê°í˜• ì…ë ¥ì—ì„œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ `resize_size`ëŠ” `[128, 128]`ì„ ê¶Œì¥í•˜ë©°, GPU ë©”ëª¨ë¦¬/ëŒ€ì—­í­ ì ˆì•½ì´ í•„ìš”í•˜ë©´ `[64, 64]`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í•´ìƒë„ë„ ê°€ëŠ¥í•˜ì§€ë§Œ ì¶©ë¶„íˆ ê²€ì¦ë˜ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤.

### ë³´ìƒ ë¶„ë¥˜ê¸° í•™ìŠµ

ë³´ìƒ ë¶„ë¥˜ê¸°ëŠ” HIL-SERLì—ì„œ ë³´ìƒ í• ë‹¹ì„ ìë™í™”í•˜ê³  ì„±ê³µ ì—¬ë¶€ë¥¼ ì‹œê°ì ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤. ë§¤ íƒ€ì„ìŠ¤í…ì— ì‚¬ëŒì´ ë³´ìƒì„ ì£¼ëŠ” ëŒ€ì‹ , ë³´ìƒ ë¶„ë¥˜ê¸°ê°€ ì„±ê³µ/ì‹¤íŒ¨ë¥¼ ì˜ˆì¸¡í•´ ì¼ê´€ëœ ë³´ìƒì„ ì œê³µí•©ë‹ˆë‹¤.

ì´ ê°€ì´ë“œëŠ” LeRobotì˜ ì‚¬ëŒ-ê°œì… RLì„ ìœ„í•œ ë³´ìƒ ë¶„ë¥˜ê¸° í•™ìŠµ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ë³´ìƒ ë¶„ë¥˜ê¸°ëŠ” ìƒíƒœì—ì„œ ë³´ìƒ ê°’ì„ ì˜ˆì¸¡í•´ RL í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Note**: ë³´ìƒ ë¶„ë¥˜ê¸° í•™ìŠµì€ ì„ íƒ ì‚¬í•­ì…ë‹ˆë‹¤. ì´ˆê¸° RL ì‹¤í—˜ì€ ê²Œì„íŒ¨ë“œ/í‚¤ë³´ë“œë¡œ ì„±ê³µ ì—¬ë¶€ë¥¼ ì§ì ‘ ë¼ë²¨ë§í•´ ì‹œì‘í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ë³´ìƒ ë¶„ë¥˜ê¸° êµ¬í˜„ì€ `modeling_classifier.py`ì— ìˆìœ¼ë©°, ì‚¬ì „í•™ìŠµ ë¹„ì „ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì´ì§„ ì„±ê³µ/ì‹¤íŒ¨ìš© ë‹¨ì¼ ì¶œë ¥ ë˜ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ì¶œë ¥ì„ ì§€ì›í•©ë‹ˆë‹¤.

**ë³´ìƒ ë¶„ë¥˜ê¸° ë°ì´í„°ì…‹ ìˆ˜ì§‘**

í•™ìŠµ ì „ì— ë¼ë²¨ëœ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•´ì•¼ í•©ë‹ˆë‹¤. `gym_manipulator.py`ì˜ `record_dataset` í•¨ìˆ˜ë¡œ ê´€ì¸¡/ì•¡ì…˜/ë³´ìƒ ë°ì´í„°ë¥¼ ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•˜ë ¤ë©´ HILSerlRobotEnvConfigì˜ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
python -m lerobot.rl.gym_manipulator --config_path src/lerobot/configs/reward_classifier_train_config.json
```

**ë°ì´í„° ìˆ˜ì§‘ ì£¼ìš” íŒŒë¼ë¯¸í„°**

- **mode**: ë£¨íŠ¸ì—ì„œ `"record"`ë¡œ ì„¤ì •
- **dataset.repo_id**: Hub ì €ì¥ì†Œ ì´ë¦„(`"hf_username/dataset_name"`)
- **dataset.num_episodes_to_record**: ê¸°ë¡í•  ì—í”¼ì†Œë“œ ìˆ˜
- **env.processor.reset.terminate_on_success**: ì„±ê³µ ì‹œ ìë™ ì¢…ë£Œ ì—¬ë¶€(ê¸°ë³¸: `true`)
- **env.fps**: ê¸°ë¡ FPS
- **dataset.push_to_hub**: Hub ì—…ë¡œë“œ ì—¬ë¶€

`env.processor.reset.terminate_on_success`ë¥¼ `false`ë¡œ ë‘ë©´ ì„±ê³µ ì´í›„ì—ë„ ì—í”¼ì†Œë“œê°€ ê³„ì†ë˜ì–´ ë³´ìƒ=1 ì˜ˆì‹œë¥¼ ë” ë§ì´ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³´ìƒ ë¶„ë¥˜ê¸° í•™ìŠµì— ì¤‘ìš”í•©ë‹ˆë‹¤. ì¼ë°˜ HIL-SERL í•™ìŠµì—ì„œëŠ” `true`ë¥¼ ìœ ì§€í•´ ì„±ê³µ ì‹œ ìë™ ì¢…ë£Œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ì˜ˆì‹œ ì„¤ì •:

```json
{
  "env": {
    "type": "gym_manipulator",
    "name": "real_robot",
    "fps": 10,
    "processor": {
      "reset": {
        "reset_time_s": 5.0,
        "control_time_s": 20.0,
        "terminate_on_success": false
      },
      "gripper": {
        "use_gripper": true
      }
    },
    "robot": {
      // ... robot configuration ...
    },
    "teleop": {
      // ... teleoperator configuration ...
    }
  },
  "dataset": {
    "repo_id": "hf_username/dataset_name",
    "dataset_root": "data/your_dataset",
    "task": "reward_classifier_task",
    "num_episodes_to_record": 20,
    "replay_episode": null,
    "push_to_hub": true
  },
  "mode": "record",
  "device": "cpu"
}
```

**ë³´ìƒ ë¶„ë¥˜ê¸° ì„¤ì •**

ë³´ìƒ ë¶„ë¥˜ê¸°ëŠ” `configuration_classifier.py`ì—ì„œ ì„¤ì •í•©ë‹ˆë‹¤. ì£¼ìš” íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **model_name**: ê¸°ë³¸ ëª¨ë¸(ì˜ˆ: ì£¼ë¡œ `"helper2424/resnet10"`)
- **model_type**: `"cnn"` ë˜ëŠ” `"transformer"`
- **num_cameras**: ì¹´ë©”ë¼ ìˆ˜
- **num_classes**: ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜(ë³´í†µ 2)
- **hidden_dim**: ì€ë‹‰ ì°¨ì›
- **dropout_rate**: ì •ê·œí™” íŒŒë¼ë¯¸í„°
- **learning_rate**: í•™ìŠµë¥ 

ë³´ìƒ ë¶„ë¥˜ê¸° í•™ìŠµ ì˜ˆì‹œ ì„¤ì •([reward_classifier_train_config.json](https://huggingface.co/datasets/aractingi/lerobot-example-config-files/blob/main/reward_classifier_train_config.json)):

```json
{
  "policy": {
    "type": "reward_classifier",
    "model_name": "helper2424/resnet10",
    "model_type": "cnn",
    "num_cameras": 2,
    "num_classes": 2,
    "hidden_dim": 256,
    "dropout_rate": 0.1,
    "learning_rate": 1e-4,
    "device": "cuda",
    "use_amp": true,
    "input_features": {
      "observation.images.front": {
        "type": "VISUAL",
        "shape": [3, 128, 128]
      },
      "observation.images.side": {
        "type": "VISUAL",
        "shape": [3, 128, 128]
      }
    }
  }
}
```

**ë¶„ë¥˜ê¸° í•™ìŠµ**

ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤:

```bash
lerobot-train --config_path path/to/reward_classifier_train_config.json
```

**ëª¨ë¸ ë°°í¬ ë° í…ŒìŠ¤íŠ¸**

í•™ìŠµí•œ ë³´ìƒ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `HILSerlRobotEnvConfig`ì— ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤:

```python
config = GymManipulatorConfig(
    env=HILSerlRobotEnvConfig(
        processor=HILSerlProcessorConfig(
            reward_classifier=RewardClassifierConfig(
                pretrained_path="path_to_your_pretrained_trained_model"
            )
        ),
        # Other environment parameters
    ),
    dataset=DatasetConfig(...),
    mode=None  # For training
)
```

ë˜ëŠ” JSON ì„¤ì •ì—ì„œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```json
{
  "env": {
    "processor": {
      "reward_classifier": {
        "pretrained_path": "path_to_your_pretrained_model",
        "success_threshold": 0.7,
        "success_reward": 1.0
      },
      "reset": {
        "terminate_on_success": true
      }
    }
  }
}
```

`gym_manipulator.py`ë¡œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

```bash
python -m lerobot.rl.gym_manipulator --config_path path/to/env_config.json
```

ë³´ìƒ ë¶„ë¥˜ê¸°ê°€ ì¹´ë©”ë¼ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ ë³´ìƒì„ ì œê³µí•©ë‹ˆë‹¤.

**ë³´ìƒ ë¶„ë¥˜ê¸° í•™ìŠµ ì˜ˆì‹œ ì›Œí¬í”Œë¡œ**

1. **ì„¤ì • íŒŒì¼ ìƒì„±**: ë³´ìƒ ë¶„ë¥˜ê¸° ë° í™˜ê²½ ì„¤ì • JSONì„ ë§Œë“­ë‹ˆë‹¤. ì˜ˆì‹œëŠ” [ì—¬ê¸°](https://huggingface.co/datasets/lerobot/config_examples/resolve/main/reward_classifier/config.json) ì°¸ê³ .

2. **ë°ì´í„° ìˆ˜ì§‘**:

   ```bash
   python -m lerobot.rl.gym_manipulator --config_path src/lerobot/configs/env_config.json
   ```

3. **ë¶„ë¥˜ê¸° í•™ìŠµ**:

   ```bash
   lerobot-train --config_path src/lerobot/configs/reward_classifier_train_config.json
   ```

4. **ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸**:
   ```bash
   python -m lerobot.rl.gym_manipulator --config_path src/lerobot/configs/env_config.json
   ```

### Actor-Learner í•™ìŠµ

LeRobotì€ ë¶„ì‚° actor-learner ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¡œë´‡ ìƒí˜¸ì‘ìš©ê³¼ í•™ìŠµì„ ë¶„ë¦¬í•´ ë™ì‹œì— ì‹¤í–‰ë˜ë©° ì„œë¡œë¥¼ ë¸”ë¡œí‚¹í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. actor ì„œë²„ê°€ ë¡œë´‡ ê´€ì¸¡/ì•¡ì…˜ì„ ì²˜ë¦¬í•˜ê³  ë°ì´í„°ë¥¼ learner ì„œë²„ë¡œ ì „ì†¡í•˜ë©°, learnerëŠ” ê·¸ë¼ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•˜ê³  actorì— ì •ì±… ê°€ì¤‘ì¹˜ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤. í•™ìŠµì„ ìœ„í•´ learnerì™€ actor ë‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

**ì„¤ì • ì¤€ë¹„**

í•™ìŠµ ì„¤ì • íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤(ì˜ˆ: [train_config.json](https://huggingface.co/datasets/lerobot/config_examples/resolve/main/rl/train_config.json)). í•™ìŠµ ì„¤ì •ì€ `lerobot/configs/train.py`ì˜ `TrainRLServerPipelineConfig`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

1. ì •ì±… ì„¤ì •(`type="sac"`, `device` ë“±) êµ¬ì„±
2. `dataset`ì— í¬ë¡­ëœ ë°ì´í„°ì…‹ ì§€ì •
3. í¬ë¡­ íŒŒë¼ë¯¸í„° í¬í•¨í•œ í™˜ê²½ ì„¤ì •
4. SAC ê´€ë ¨ íŒŒë¼ë¯¸í„°ëŠ” [configuration_sac.py](https://github.com/huggingface/lerobot/blob/main/src/lerobot/policies/sac/configuration_sac.py#L79) ì°¸ê³ 
5. `policy`ì˜ `input_features`/`output_features`ê°€ íƒœìŠ¤í¬ì— ë§ëŠ”ì§€ í™•ì¸

**Learner ì‹œì‘**

ë¨¼ì € learner ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
python -m lerobot.rl.learner --config_path src/lerobot/configs/train_config_hilserl_so100.json
```

learnerëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- ì •ì±… ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
- ë¦¬í”Œë ˆì´ ë²„í¼ ì¤€ë¹„
- actorì™€ í†µì‹ í•˜ëŠ” `gRPC` ì„œë²„ ì˜¤í”ˆ
- ì „ì´(transition) ì²˜ë¦¬ ë° ì •ì±… ì—…ë°ì´íŠ¸

**Actor ì‹œì‘**

ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ê°™ì€ ì„¤ì •ìœ¼ë¡œ actorë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
python -m lerobot.rl.actor --config_path src/lerobot/configs/train_config_hilserl_so100.json
```

actorëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- `gRPC`ë¡œ learnerì— ì—°ê²°
- í™˜ê²½ ì´ˆê¸°í™”
- ì •ì±… ë¡¤ì•„ì›ƒ ìˆ˜í–‰
- ì „ì´ë¥¼ learnerë¡œ ì „ì†¡
- ì—…ë°ì´íŠ¸ëœ ì •ì±… íŒŒë¼ë¯¸í„° ìˆ˜ì‹ 

**í•™ìŠµ íë¦„**

1. actorê°€ í™˜ê²½ì—ì„œ ì •ì±… ì‹¤í–‰
2. ì „ì´ë¥¼ ìˆ˜ì§‘í•´ learnerë¡œ ì „ì†¡
3. learnerê°€ ì „ì´ë¡œ ì •ì±… ì—…ë°ì´íŠ¸
4. ì—…ë°ì´íŠ¸ëœ íŒŒë¼ë¯¸í„°ë¥¼ actorì— ì „ë‹¬
5. ì„¤ì •ëœ ìŠ¤í…ê¹Œì§€ ë°˜ë³µ

**Human in the Loop**

- íš¨ìœ¨ì  í•™ìŠµì„ ìœ„í•´ ì¸ê°„ ê°œì…ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ìœ„í—˜ ìƒí™©ì—ì„œ ë³´ì •í•˜ê³  íƒìƒ‰ì„ ë•ìŠµë‹ˆë‹¤.
- ê°œì…í•˜ë ¤ë©´ ê²Œì„íŒ¨ë“œì˜ ì˜¤ë¥¸ìª½ ìƒë‹¨ íŠ¸ë¦¬ê±°(ë˜ëŠ” í‚¤ë³´ë“œ `space`)ë¥¼ ëˆ„ë¦…ë‹ˆë‹¤. ì •ì±… ì•¡ì…˜ì´ ì¼ì‹œ ì •ì§€ë˜ë©° ì‚¬ëŒì´ ì œì–´í•©ë‹ˆë‹¤.
- í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ ê°œì… íšŸìˆ˜ê°€ ê°ì†Œí•˜ëŠ” ê²ƒì´ ì„±ê³µì ì¸ ì‹¤í—˜ì…ë‹ˆë‹¤. `wandb` ëŒ€ì‹œë³´ë“œì—ì„œ ê°œì…ë¥ ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Example showing how human interventions help guide policy learning over time

- ê·¸ë˜í”„ëŠ” ìƒí˜¸ì‘ìš© ìŠ¤í…ì— ë”°ë¥¸ ì—í”¼ì†Œë“œ ë³´ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- ì£¼í™©ìƒ‰ì€ ì¸ê°„ ê°œì…ì´ ì—†ëŠ” ì‹¤í—˜, ë¶„í™/íŒŒë€ìƒ‰ì€ ê°œì…ì´ ìˆëŠ” ì‹¤í—˜ì…ë‹ˆë‹¤.
- ì¸ê°„ ê°œì…ì´ ìˆì„ ë•Œ ìµœëŒ€ ë³´ìƒ ë„ë‹¬ê¹Œì§€ ìŠ¤í… ìˆ˜ê°€ ì•½ 1/4ë¡œ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…**

ì„¤ì •ì—ì„œ `wandb.enable`ì„ `true`ë¡œ ë‘ë©´ [Weights & Biases](https://wandb.ai/site/) ëŒ€ì‹œë³´ë“œì—ì„œ í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì¸ê°„ ê°œì… ê°€ì´ë“œ

í•™ìŠµì€ ê°œì… ì „ëµì— ë¯¼ê°í•˜ë¯€ë¡œ ëª‡ ë²ˆì˜ ì‹œí–‰ì°©ì˜¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- í•™ìŠµ ì´ˆë°˜ì—ëŠ” ëª‡ ì—í”¼ì†Œë“œ ë™ì•ˆ ì •ì±…ì´ íƒìƒ‰í•˜ë„ë¡ ë‘ì„¸ìš”.
- ê¸´ ì‹œê°„ ë™ì•ˆ ê°œì…í•˜ì§€ ë§ê³ , ë¡œë´‡ì´ ì˜ëª»ëœ ë°©í–¥ìœ¼ë¡œ ê°ˆ ë•Œ ì§§ê²Œ êµì •í•˜ì„¸ìš”.
- ì •ì±…ì´ ì‘ì—…ì„ ë‹¬ì„±í•˜ê¸° ì‹œì‘í•˜ë©´, ì§§ê³  ê°„ë‹¨í•œ ë³´ì •(ì˜ˆ: ì§§ì€ ê·¸ë¦½ ì¡°ì‘)ë§Œìœ¼ë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤.

ì´ìƒì ì¸ í–‰ë™ì€ ì•„ë˜ ê·¸ë˜í”„ì²˜ëŸ¼ ê°œì…ë¥ ì´ ì ì°¨ ê°ì†Œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

Plot of the intervention rate during a training run on a pick and lift cube task

### íŠœë‹í•´ì•¼ í•  í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°

í•™ìŠµ ì•ˆì •ì„±ê³¼ ì†ë„ì— í° ì˜í–¥ì„ ì£¼ëŠ” íŒŒë¼ë¯¸í„°ê°€ ìˆìŠµë‹ˆë‹¤:

- **`temperature_init`** (`policy.temperature_init`) â€“ SACì˜ ì´ˆê¸° ì—”íŠ¸ë¡œí”¼ ì˜¨ë„. ë†’ì„ìˆ˜ë¡ íƒìƒ‰ì´ ëŠ˜ê³  ë‚®ì„ìˆ˜ë¡ ì´ˆê¸°ì— ê²°ì •ì ì…ë‹ˆë‹¤. ì‹œì‘ ê°’ìœ¼ë¡œ `1e-2`ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. ë„ˆë¬´ ë†’ìœ¼ë©´ ì¸ê°„ ê°œì… íš¨ê³¼ê°€ ì¤„ê³  í•™ìŠµì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **`policy_parameters_push_frequency`** (`policy.actor_learner_config.policy_parameters_push_frequency`) â€“ learnerê°€ actorë¡œ ê°€ì¤‘ì¹˜ë¥¼ í‘¸ì‹œí•˜ëŠ” ê°„ê²©(ì´ˆ). ê¸°ë³¸ê°’ì€ `4 s`. ë” ì‹ ì„ í•œ ê°€ì¤‘ì¹˜ë¥¼ ì›í•˜ë©´ **1~2 s**ë¡œ ì¤„ì´ë˜ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. ì—°ê²°ì´ ëŠë¦´ ë•Œë§Œ ëŠ˜ë¦¬ì„¸ìš”.
- **`storage_device`** (`policy.storage_device`) â€“ learnerê°€ ì •ì±… íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•˜ëŠ” ì¥ì¹˜. ì—¬ìœ  GPU ë©”ëª¨ë¦¬ê°€ ìˆìœ¼ë©´ ê¸°ë³¸ `"cpu"` ëŒ€ì‹  `"cuda"`ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. GPUì— ë‘ë©´ CPUâ†’GPU ì „ì†¡ ì˜¤ë²„í—¤ë“œê°€ ì¤„ì–´ ì—…ë°ì´íŠ¸ ì†ë„ê°€ ê°œì„ ë©ë‹ˆë‹¤.

ì¶•í•˜í•©ë‹ˆë‹¤ ğŸ‰ íŠœí† ë¦¬ì–¼ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!

> [!TIP]
> ì§ˆë¬¸ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ë©´ [Discord](https://discord.com/invite/s3KuuzsPFb)ì— ë¬¸ì˜í•˜ì„¸ìš”.

ë…¼ë¬¸ ì¸ìš©:

```
@article{luo2024precise,
  title={Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning},
  author={Luo, Jianlan and Xu, Charles and Wu, Jeffrey and Levine, Sergey},
  journal={arXiv preprint arXiv:2410.21845},
  year={2024}
}
```
